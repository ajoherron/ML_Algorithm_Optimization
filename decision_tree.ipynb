{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1682784585965,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "A3Rx5bOToH8f"
   },
   "source": [
    "# Decision Tree Modeling from Scratch with Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Libraries/Packages, Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexherron/opt/miniconda3/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import numba as nb\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "from concurrent.futures import as_completed\n",
    "from joblib import Parallel, delayed\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2145,
     "status": "ok",
     "timestamp": 1682784612050,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "pR4Vz03RoQUS"
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "df = pd.read_csv('cleaned_bullying.csv')\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop(columns=['Unnamed: 0',\n",
    "                      'Bullied_on_school_property_in_past_12_months',\n",
    "                      'Bullied_not_on_school_property_in_past_12_months',\n",
    "                      'Cyber_bullied_in_past_12_months'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1682715969414,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "6RS4HEg3ptuQ"
   },
   "source": [
    "# 1. Decision Tree Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411313,
     "status": "ok",
     "timestamp": 1682717729987,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "pmkQGIP0wsJe",
    "outputId": "e4768223-0b6a-4181-ccec-68a45d1d0359"
   },
   "outputs": [],
   "source": [
    "# Brute force approach:\n",
    "# Generate all possible binary splits of the data\n",
    "# using itertools.combinations\n",
    "def generate_possible_splits(data):\n",
    "    \n",
    "    # Get all features (aside from last column)\n",
    "    features = list(data.columns)[:-1]\n",
    "    \n",
    "    # Initialize splits\n",
    "    splits = []\n",
    "    \n",
    "    # Loop through each feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # Get unique values for current feature\n",
    "        values = data[feature].unique()\n",
    "        \n",
    "        # Loop through different split sizes\n",
    "        for i in range(1, len(values)):\n",
    "            \n",
    "            # Generate all combinations using itertools, \n",
    "            # add to splits list\n",
    "            for split in itertools.combinations(values, i):\n",
    "                splits.append((feature, split))\n",
    "                \n",
    "    return splits\n",
    "\n",
    "# Evaluate each split based on entropy / information gain,\n",
    "# where information gain is the change in entropy\n",
    "def evaluate_split(split, data):\n",
    "    \n",
    "    # Unpack features and values from split\n",
    "    feature, values = split\n",
    "    \n",
    "    # Calculate number of instances\n",
    "    total = len(data)\n",
    "    \n",
    "    # Split data into left/right subsets\n",
    "    # based on feature values\n",
    "    left = data[data[feature].isin(values)]\n",
    "    right = data[~data[feature].isin(values)]\n",
    "    \n",
    "    # Calculate proportion of instances in \n",
    "    # left/right subsets\n",
    "    left_proportion = len(left) / total\n",
    "    right_proportion = len(right) / total\n",
    "    \n",
    "    # Calculate entropy before/after split and info gain,\n",
    "    # which is the difference of the two\n",
    "    entropy_before = calculate_entropy(data)\n",
    "    entropy_after = (left_proportion * calculate_entropy(left) +\n",
    "                     right_proportion * calculate_entropy(right))\n",
    "    information_gain = entropy_before - entropy_after\n",
    "    \n",
    "    # Return info gain and left/right subsets\n",
    "    return information_gain, left, right\n",
    "\n",
    "# Calculate entropy of data\n",
    "# for evaluate_split() function\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    # Calculate number of instances\n",
    "    total = len(data)\n",
    "    \n",
    "    # Count occurences for each label in last column\n",
    "    counts = data.iloc[:, -1].value_counts()\n",
    "    \n",
    "    # Initialize entropy\n",
    "    entropy = 0\n",
    "    \n",
    "    # Calculate entropy for current class label\n",
    "    # entropy = -sum(p * log_2(p))\n",
    "    for count in counts:\n",
    "        proportion = count / total\n",
    "        entropy -= proportion * math.log2(proportion)\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "# Build decision tree using brute force\n",
    "def build_decision_tree(data):\n",
    "    \n",
    "    # If all instances have the same class, \n",
    "    # create leaf node with class label\n",
    "    if len(data.iloc[:, -1].unique()) == 1:\n",
    "        return {'class': data.iloc[0, -1]}\n",
    "    \n",
    "    # Initalize best split and best gain\n",
    "    best_split = None\n",
    "    best_gain = 0\n",
    "    \n",
    "    # Loop over all possible splits\n",
    "    for split in generate_possible_splits(data):\n",
    "        \n",
    "        # Evaluate information gain for each split\n",
    "        gain, left, right = evaluate_split(split, data)\n",
    "        \n",
    "        # Update best split if current split has greater\n",
    "        # information gain\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_split = split\n",
    "            best_left = left\n",
    "            best_right = right\n",
    "            \n",
    "    # If no split improved information gain,\n",
    "    # create leaf node with majority class label\n",
    "    if best_split is None:\n",
    "        return {'class': data.iloc[:, -1].value_counts().idxmax()}\n",
    "    else:\n",
    "        \n",
    "        # Recursively call build_decision_tree function\n",
    "        # to build left and right subtrees\n",
    "        left_subtree = build_decision_tree(best_left)\n",
    "        right_subtree = build_decision_tree(best_right)\n",
    "        \n",
    "        # Create decision node with best split feature / value,\n",
    "        # including left/right subtrees\n",
    "        return {'feature': best_split[0],\n",
    "                'value': best_split[1],\n",
    "                'left': left_subtree,\n",
    "                'right': right_subtree}\n",
    "\n",
    "# Calculate y_pred to compare with y_true\n",
    "def predict(instance, tree):    \n",
    "    \n",
    "    # If current node is a leaf node\n",
    "    if 'class' in tree:\n",
    "        # Return predicted class\n",
    "        return tree['class']\n",
    "    \n",
    "    else:\n",
    "        feature = tree['feature']\n",
    "        value = tree['value']\n",
    "        \n",
    "        # If value of feature satisfies condition\n",
    "        # add to left subtree\n",
    "        if instance[feature] in value:\n",
    "            subtree = tree['left']\n",
    "            \n",
    "        # Otherwise, add to right subtree\n",
    "        else:\n",
    "            subtree = tree['right']\n",
    "            \n",
    "        # Call function recursively until a leaf \n",
    "        # node is reached\n",
    "        return predict(instance, subtree)\n",
    "    \n",
    "# Single function to split data, train decision tree model,\n",
    "# evaluate model, and print evaluation metrics \n",
    "def evaluate():\n",
    "    \n",
    "    # Load dataset, split into train / test sets\n",
    "    data = df\n",
    "    train_data = data.sample(frac=0.8, random_state=1)\n",
    "    test_data = data.drop(train_data.index)\n",
    "\n",
    "    # Train decision tree model \n",
    "    # (calculate time required to train)\n",
    "    start = time.time()\n",
    "    tree = build_decision_tree(train_data)\n",
    "    end = time.time()\n",
    "    time_diff = end - start\n",
    "\n",
    "    # Evaluate decision tree\n",
    "    y_true = test_data.iloc[:, -1]\n",
    "    y_pred = test_data.apply(lambda x: predict(x, tree), axis=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred))\n",
    "\n",
    "    # Print results\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Time: {time_diff}')\n",
    "    \n",
    "    # Make beeping sounds to notify end of evaluation\n",
    "    beep = lambda x: os.system(\"echo '\\a';sleep 0.5;\" * x)\n",
    "    beep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5920597098471023\n",
      "Accuracy: 0.6158166363084396\n",
      "Time: 289.3000137805939\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evalute Decision Tree Model Using Line Profiler\n",
    "\n",
    "~96% of time spent on evaluate_split(split, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load copy of dataframe\n",
    "data = df\n",
    "\n",
    "# Train/test split\n",
    "train_data = data.sample(frac=0.8, random_state=1)\n",
    "test_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f build_decision_tree tree = build_decision_tree(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Random Binary Splits\n",
    "\n",
    "- Avoid calculating info gain for all possible splits (brute force)\n",
    "- Instead, choose random subset of features to split on (reducing number of splits that need to be evaluted)\n",
    "- Implemented by changing 2 functions: \n",
    "    - generate_possible_splits\n",
    "    - evaluate_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79698,
     "status": "ok",
     "timestamp": 1682718469163,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "nZ1Fb6ca0kvC",
    "outputId": "aa398909-5c6d-4750-c9c2-f9f63b523c36"
   },
   "outputs": [],
   "source": [
    "# Main idea: generate random binary splits of the data:\n",
    "# Randomly sample a subset of the features to split on\n",
    "# This reduces the number of splits that need to be evaluated,\n",
    "# but still allows the algorithm to find a diverse range of splits.\n",
    "def generate_possible_splits(data):\n",
    "    \n",
    "    # Get all features (aside from last column)\n",
    "    # and random sample sizes\n",
    "    features = list(data.columns)[:-1]\n",
    "    rand_a, rand_b = 5, 3\n",
    "    \n",
    "    # Randomly sample features (without replacement)\n",
    "    features = list(np.random.choice(features, rand_a, replace=False))\n",
    "\n",
    "    # Initialize splits\n",
    "    splits = []\n",
    "    \n",
    "    # Loop through each feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # Get unique values for current features\n",
    "        values = data[feature].unique()\n",
    "        \n",
    "        # Randomly sample values without replacement\n",
    "        values = list(np.random.choice(values, min(rand_b, len(values)), replace=False))\n",
    "        \n",
    "        # Loop through different split sizes\n",
    "        for i in range(1, len(values)):\n",
    "            \n",
    "            # Generate all combinations using itertools, \n",
    "            # add to splits list\n",
    "            for split in itertools.combinations(values, i):\n",
    "                splits.append((feature, split))\n",
    "                \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.602041000199365\n",
      "Accuracy: 0.6317547055251973\n",
      "Time: 54.09505915641785\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize Decision Trees with Threading\n",
    "\n",
    "### 4.a. Threading for evaluate_split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using threading (with 4 threads)\n",
    "def evaluate_split(split, data, num_threads=4):\n",
    "    \n",
    "    # Unpack features and values from split\n",
    "    feature, values = split\n",
    "\n",
    "    # Calculate number of instances\n",
    "    total = len(data)\n",
    "    \n",
    "    # Split data into left/right subsets\n",
    "    # based on feature values\n",
    "    left = data[data[feature].isin(values)]\n",
    "    right = data[~data[feature].isin(values)]\n",
    "    \n",
    "    # If left or right subset is empty,\n",
    "    # return negative infinity (for no info gain)\n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        return -float('inf'), None, None\n",
    "    \n",
    "    # Calculate proportion of instances in \n",
    "    # left/right subsets\n",
    "    left_proportion = len(left) / total\n",
    "    right_proportion = len(right) / total\n",
    "\n",
    "    # Use threading to calculate entropy for each subset\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        \n",
    "        # Submit entropy calculation for data, left/right subsets to executor\n",
    "        futures = [executor.submit(calculate_entropy, subset) for subset in [data, left, right]]\n",
    "        \n",
    "        # Retrieve entropy results\n",
    "        results = [f.result() for f in futures]\n",
    "    \n",
    "    # Calculate entropy before/after split and info gain,\n",
    "    # which is the difference of the two\n",
    "    entropy_before= results[0]\n",
    "    entropy_after = (left_proportion * results[1] +\n",
    "                     right_proportion * results[2])\n",
    "    information_gain = entropy_before - entropy_after\n",
    "    \n",
    "    # Return info gain and left/right subsets\n",
    "    return information_gain, left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6007529866425384\n",
      "Accuracy: 0.6323618700667881\n",
      "Time: 71.76984810829163\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b Threading for build_decision_tree() function and evaluate_split() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(data):\n",
    "\n",
    "    # If all instances have the same class, \n",
    "    # create leaf node with class label\n",
    "    if len(data.iloc[:, -1].unique()) == 1:\n",
    "        return {'class': data.iloc[0, -1]}\n",
    "    \n",
    "    # Initalize best split and best gain\n",
    "    best_split = None\n",
    "    best_gain = 0\n",
    "    \n",
    "    # Lock threading\n",
    "    split_lock = threading.Lock()\n",
    "    \n",
    "    # Initialize best left/right splits\n",
    "    best_left = None  \n",
    "    best_right = None \n",
    "    \n",
    "    # Create helper function for evaluating/updating best splits\n",
    "    def evaluate_and_update_split(split):\n",
    "        nonlocal best_split, best_gain, best_left, best_right\n",
    "        gain, left, right = evaluate_split(split, data)\n",
    "        with split_lock:\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split = split\n",
    "                best_left = left\n",
    "                best_right = right\n",
    "    \n",
    "    # Create list for threads\n",
    "    threads = []\n",
    "    \n",
    "    # Loop through each split, evaluate each concurrently\n",
    "    for split in generate_possible_splits(data):\n",
    "\n",
    "        # Make thread for each split\n",
    "        thread = threading.Thread(target=evaluate_and_update_split, args=(split,))\n",
    "\n",
    "        # Start thread, append to thread list\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Join threads when they all complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # If no split improved information gain,\n",
    "    # create leaf node with majority class label\n",
    "    if best_split is None:\n",
    "        return {'class': data.iloc[:, -1].value_counts().idxmax()}\n",
    "    else:\n",
    "        \n",
    "        # Recursively call build_decision_tree function\n",
    "        # to build left and right subtrees\n",
    "        left_subtree = build_decision_tree(best_left)\n",
    "        right_subtree = build_decision_tree(best_right)\n",
    "        \n",
    "        # Create decision node with best split feature / value,\n",
    "        # including left/right subtrees\n",
    "        return {'feature': best_split[0],\n",
    "                'value': best_split[1],\n",
    "                'left': left_subtree,\n",
    "                'right': right_subtree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6122680463753891\n",
      "Accuracy: 0.6414693381906497\n",
      "Time: 81.38614583015442\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.a Optimizing Model by Adding Variables for Number of Features and Number of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add adjustable variables for number of features \n",
    "# and number of values\n",
    "def generate_possible_splits(data, num_features, num_values):\n",
    "    \n",
    "    # Get all features (aside from last column)\n",
    "    features = list(data.columns)[:-1]\n",
    "    \n",
    "    # Randomly sample features (using num_features)\n",
    "    features = list(np.random.choice(features, num_features, replace=False))\n",
    "\n",
    "    # Initialize splits\n",
    "    splits = []\n",
    "    \n",
    "    # Loop through each feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # Get unique values for current features\n",
    "        values = data[feature].unique()\n",
    "        \n",
    "        # Randomly sample values (using num_values)\n",
    "        values = list(np.random.choice(values, min(num_values, len(values)), replace=False))\n",
    "\n",
    "        # Loop through different split sizes\n",
    "        for i in range(1, len(values)):\n",
    "            \n",
    "            # Generate all combinations using itertools, \n",
    "            # add to splits list\n",
    "            for split in itertools.combinations(values, i):\n",
    "                splits.append((feature, split))\n",
    "                \n",
    "    return splits\n",
    "\n",
    "# Use evaluate_split function without threading\n",
    "def evaluate_split(split, data):\n",
    "    \n",
    "    # Unpack features and values from split\n",
    "    feature, values = split\n",
    "    \n",
    "    # Calculate number of instances\n",
    "    total = len(data)\n",
    "    \n",
    "    # Splits data into left/right instances\n",
    "    # based on features values\n",
    "    left = data[data[feature].isin(values)]\n",
    "    right = data[~data[feature].isin(values)]\n",
    "    \n",
    "    # If left or right subset is empty,\n",
    "    # return None\n",
    "    if len(left) == 0 or len(right) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calculate proportion of instances in\n",
    "    # left/right subsets\n",
    "    left_proportion = len(left) / total\n",
    "    right_proportion = len(right) / total\n",
    "    \n",
    "    # Calculate entropy before/after split and info gain,\n",
    "    # which is the difference of the two\n",
    "    entropy_before = calculate_entropy(data)\n",
    "    entropy_after = (left_proportion * calculate_entropy(left) +\n",
    "                     right_proportion * calculate_entropy(right))\n",
    "    information_gain = entropy_before - entropy_after\n",
    "\n",
    "    # If info gain is negative, return None\n",
    "    if information_gain <= 0:\n",
    "        return None\n",
    "    \n",
    "    # Return info gain and left/right subsets\n",
    "    return information_gain, left, right, split\n",
    "\n",
    "\n",
    "# Use build_decision_tree function without threading\n",
    "def build_decision_tree(data, num_features, num_values):\n",
    "\n",
    "    # If all instances have the same class, \n",
    "    # create leaf node with class label\n",
    "    if len(data.iloc[:, -1].unique()) == 1:\n",
    "        return {'class': data.iloc[0, -1]}\n",
    "    \n",
    "    # Generate possibel splits with adjustable numbers of features/values\n",
    "    splits = generate_possible_splits(data, num_features, num_values)\n",
    "    \n",
    "    # Evaluate each split, store those with non-zero info gain\n",
    "    splits_gain = [split_gain for split_gain in (evaluate_split(split, data) for split in splits) if split_gain]\n",
    "    \n",
    "    # If no split improves info gain, return leaf node with majority class\n",
    "    if not splits_gain:\n",
    "        return {'class': data.iloc[:, -1].value_counts().idxmax()}\n",
    "\n",
    "    # Find best split (highest info gain)\n",
    "    best_gain, best_left, best_right, best_split = max(splits_gain, key=lambda x: x[0])\n",
    "\n",
    "    # Build left/right trees recursively\n",
    "    left_subtree = build_decision_tree(best_left, num_features, num_values)\n",
    "    right_subtree = build_decision_tree(best_right, num_features, num_values)\n",
    "\n",
    "    # Return node with best split information\n",
    "    return {'feature': best_split[0],\n",
    "            'value': best_split[1],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree}\n",
    "    \n",
    "# Get rid of beeping, allow for changing number of \n",
    "# features and values\n",
    "def evaluate(num_feat, num_val):\n",
    "\n",
    "    # Load dataset, split into train / test sets\n",
    "    data = df\n",
    "    train_data = data.sample(frac=0.8, random_state=1)\n",
    "    test_data = data.drop(train_data.index)\n",
    "\n",
    "    # Train decision tree model \n",
    "    # (calculate time required to train)    \n",
    "    start = time.time()\n",
    "    tree = build_decision_tree(train_data, num_features=num_feat, num_values=num_val)\n",
    "    end = time.time()\n",
    "    time_diff = end - start\n",
    "\n",
    "    # Evaluate decision tree\n",
    "    y_true = test_data.iloc[:, -1]\n",
    "    y_pred = test_data.apply(lambda x: predict(x, tree), axis=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred))\n",
    "\n",
    "    # Print results\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Time: {time_diff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6068284463324489\n",
      "Accuracy: 0.6419247115968427\n",
      "Time: 25.457530736923218\n"
     ]
    }
   ],
   "source": [
    "evaluate(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.b Basic Hyperparameter Tuning (Number of Features & Number of Values\n",
    "\n",
    "- num_features, num_values = 2, 2 appears to have the best balance of predicting power and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1\n",
      "Number of values: 1\n",
      "AUC: 0.5\n",
      "Accuracy: 0.5980570734669095\n",
      "Time: 0.0012850761413574219\n",
      "\n",
      "\n",
      "Number of features: 1\n",
      "Number of values: 2\n",
      "AUC: 0.5725416749735457\n",
      "Accuracy: 0.6396478445658773\n",
      "Time: 0.39314794540405273\n",
      "\n",
      "\n",
      "Number of features: 1\n",
      "Number of values: 3\n",
      "AUC: 0.5781940589200547\n",
      "Accuracy: 0.6435944140862173\n",
      "Time: 0.5821387767791748\n",
      "\n",
      "\n",
      "Number of features: 2\n",
      "Number of values: 1\n",
      "AUC: 0.5\n",
      "Accuracy: 0.5980570734669095\n",
      "Time: 0.0013942718505859375\n",
      "\n",
      "\n",
      "Number of features: 2\n",
      "Number of values: 2\n",
      "AUC: 0.6038546475071696\n",
      "Accuracy: 0.6510321797207043\n",
      "Time: 4.513548135757446\n",
      "\n",
      "\n",
      "Number of features: 2\n",
      "Number of values: 3\n",
      "AUC: 0.5991628582820863\n",
      "Accuracy: 0.6425318761384335\n",
      "Time: 11.353196144104004\n",
      "\n",
      "\n",
      "Number of features: 3\n",
      "Number of values: 1\n",
      "AUC: 0.5\n",
      "Accuracy: 0.5980570734669095\n",
      "Time: 0.00164794921875\n",
      "\n",
      "\n",
      "Number of features: 3\n",
      "Number of values: 2\n",
      "AUC: 0.6040446194426979\n",
      "Accuracy: 0.6451123254401943\n",
      "Time: 12.749154090881348\n",
      "\n",
      "\n",
      "Number of features: 3\n",
      "Number of values: 3\n",
      "AUC: 0.6030569954146028\n",
      "Accuracy: 0.645264116575592\n",
      "Time: 22.329774141311646\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect permutations of all parameters below 4\n",
    "num_features_values_list = [1, 1, 2, 2, 3, 3]\n",
    "permuts = itertools.permutations(num_features_values_list, 2)\n",
    "param_list = []\n",
    "for i in permuts:\n",
    "    if i not in param_list:\n",
    "        param_list.append(i)\n",
    "    \n",
    "# Evaluate each number of features / number of values pair\n",
    "for param_pair in param_list:\n",
    "    print(f'Number of features: {param_pair[0]}')\n",
    "    print(f'Number of values: {param_pair[1]}')\n",
    "    evaluate(param_pair[0], param_pair[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrjaUf0u1X6u15lYYZoS4H",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
