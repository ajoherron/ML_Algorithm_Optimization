{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1682784585965,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "A3Rx5bOToH8f"
   },
   "source": [
    "# Logistic Regression Modeling from Scratch with Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Libraries/Packages, Load Data, Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import numba as nb\n",
    "import multiprocessing as mp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import wait\n",
    "from concurrent.futures import as_completed\n",
    "from joblib import Parallel, delayed\n",
    "from numba import njit\n",
    "from numba import jit\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2145,
     "status": "ok",
     "timestamp": 1682784612050,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "pR4Vz03RoQUS"
   },
   "outputs": [],
   "source": [
    "# Load in data\n",
    "df = pd.read_csv('cleaned_bullying.csv')\n",
    "\n",
    "# Drop columns\n",
    "df = df.drop(columns=['Unnamed: 0',\n",
    "                      'Bullied_on_school_property_in_past_12_months',\n",
    "                      'Bullied_not_on_school_property_in_past_12_months',\n",
    "                      'Cyber_bullied_in_past_12_months'])\n",
    "\n",
    "# Split into X and y\n",
    "X = df.drop('bullied', axis=1).values\n",
    "y = df['bullied'].values\n",
    "\n",
    "# Load copy of dataframe\n",
    "data = df\n",
    "\n",
    "# Train/test split\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for learning rate, number of iterations, and batch size\n",
    "learning_rate = 0.0001\n",
    "number_iterations = 100_000\n",
    "batch_num = 50\n",
    "digits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1682715969414,
     "user": {
      "displayName": "Alex Herron",
      "userId": "01849162412288078755"
     },
     "user_tz": 240
    },
    "id": "6RS4HEg3ptuQ"
   },
   "source": [
    "# 1. Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    # Initialize model\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    # Add intercept term (bias) to X\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    # Activation function\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Logistic loss\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    # Fit model to training data\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Add intercept term to X if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            # Calculate dot product, apply sigmoid activation\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            \n",
    "            # Calculate gradient, update weights using\n",
    "            # gradient descent\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "            # Compute predictions after every 10000 iterations\n",
    "            if i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "    \n",
    "    # Predict probabilities (for class 1)\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Add intercept term to X if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    # Predict class labels from 0.5 threshold\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "    \n",
    "# Function for training model and making predictions\n",
    "def run_logistic_regression():\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    # Run and time model\n",
    "    start = time.time()\n",
    "    y_pred = run_logistic_regression()\n",
    "    end = time.time()\n",
    "    time_diff = round(end - start, digits)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    auc = round(roc_auc_score(y_test, y_pred), digits)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), digits)\n",
    "    precision = round(precision_score(y_test, y_pred), digits)\n",
    "    recall = round(recall_score(y_test, y_pred), digits)\n",
    "    f1 = round(f1_score(y_test, y_pred), digits)\n",
    "\n",
    "    # Print Results\n",
    "    print(f'Time: {time_diff}')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'AUC: {recall}')\n",
    "    print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50.67245\n",
      "AUC: 0.62521\n",
      "Accuracy: 0.67092\n",
      "Precision: 0.63946\n",
      "AUC: 0.39985\n",
      "F1: 0.49203\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimize Using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # Initialize model\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    # Add intercept (bias) to X\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    # Activation function\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Logistic loss\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "    # Optimized random batch selection\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Add intercept term to X if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize weights with zeros\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent optimization with \n",
    "        # random batch selection\n",
    "        rng = np.random.default_rng()\n",
    "        idx = rng.integers(len(X), size=(self.num_iter, 10))\n",
    "        for i in range(self.num_iter):\n",
    "            batch_idx = idx[i]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            z = np.dot(batch_X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            # Compute predictions after every 10000 iterations\n",
    "            if i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "    \n",
    "    # Predict probabilities (class 1)\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Add intercept term to X if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    # Predict class labels from 0.5 threshold\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "    \n",
    "# Function for training model and making predictions\n",
    "def run_logistic_regression():\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations,\n",
    "                            batch_size=batch_num)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.02914\n",
      "AUC: 0.62165\n",
      "Accuracy: 0.66879\n",
      "Precision: 0.63875\n",
      "AUC: 0.38919\n",
      "F1: 0.48367\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimize By Normalizing Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # Initialize model\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    # Add intercept (bias) to X\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    # Normalize data\n",
    "    def __normalize(self, X):\n",
    "        X_norm = X.copy()\n",
    "        self.mean = np.mean(X_norm, axis=0)\n",
    "        self.std = np.std(X_norm, axis=0)\n",
    "        X_norm = (X_norm - self.mean) / (self.std + 1e-8)\n",
    "        return X_norm\n",
    "    \n",
    "    # Activation function\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Logistic Loss\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    # optimized random batch selection\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Normalize the input features\n",
    "        X = self.__normalize(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        rng = np.random.default_rng()\n",
    "        idx = rng.integers(len(X), size=(self.num_iter, 10))\n",
    "        for i in range(self.num_iter):\n",
    "            batch_idx = idx[i]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            z = np.dot(batch_X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            # Compute predictions after every 10000 iterations\n",
    "            if i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "    \n",
    "    # Predict probabilities (class 1)\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Add intercept term if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    # Convert probabilities to predictions\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "# Fit model and make predictions\n",
    "def run_logistic_regression():\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations,\n",
    "                            batch_size=batch_num)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.22017\n",
      "AUC: 0.63071\n",
      "Accuracy: 0.66834\n",
      "Precision: 0.61624\n",
      "AUC: 0.44516\n",
      "F1: 0.51691\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimize with Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # Initialize model\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5, num_threads=4):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_threads = num_threads\n",
    "        \n",
    "    # Add intercept term (bias) to X\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    # Activation function\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Logistic loss\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        \n",
    "    # Batch gradient descent for batch of data\n",
    "    def __batch_gradient_descent(self, batch_X, batch_y):\n",
    "        z = np.dot(batch_X, self.theta)\n",
    "        h = self.__sigmoid(z)\n",
    "        gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "        self.theta -= self.lr * gradient\n",
    "    \n",
    "    # Fit model to data\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # Add intercept term to X if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        # Initialize weights with zeros\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        # Create thread pool\n",
    "        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            for i in range(self.num_iter):\n",
    "                \n",
    "                # Generate random batch indices\n",
    "                batch_idx = np.random.randint(len(X) - self.batch_size + 1, size=self.num_threads)\n",
    "                batch_idx = [idx + j for j in range(self.batch_size) for idx in batch_idx]\n",
    "\n",
    "                # Split data into batches\n",
    "                batches_X = np.split(X[batch_idx], self.num_threads)\n",
    "                batches_y = np.split(y[batch_idx], self.num_threads)\n",
    "\n",
    "                # Submit batches for processing\n",
    "                futures = []\n",
    "                for j in range(self.num_threads):\n",
    "                    futures.append(executor.submit(self.__batch_gradient_descent, batches_X[j], batches_y[j]))\n",
    "\n",
    "                # Wait for all threads to finish\n",
    "                for future in futures:\n",
    "                    future.result()\n",
    "\n",
    "                # Compute predictions after every 10000 iterations\n",
    "                if i % 10000 == 0:\n",
    "                    z = np.dot(X, self.theta)\n",
    "                    h = self.__sigmoid(z)\n",
    "    \n",
    "    # Predict probability (class 1)\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Add intercept to X if required \n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    # Convert probabilities to predictions\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 37.76367\n",
      "AUC: 0.62811\n",
      "Accuracy: 0.67046\n",
      "Precision: 0.63022\n",
      "AUC: 0.41927\n",
      "F1: 0.50354\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f LogisticRegression.fit run_logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimize with Numba / CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # Initialize model\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, random_state=13, batch_size=5, verbose=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    # Add intercept (bias) to X\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    # Activation function\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Logistic loss\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    # Gradient descent using numba\n",
    "    def fit_numba(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        self.theta = self.gradient_descent(X, y, np.zeros(X.shape[1]), self.lr, self.num_iter)\n",
    "        \n",
    "    # Perform gradient descent using Numba\n",
    "    @staticmethod    \n",
    "    @jit(nopython=True)\n",
    "    def gradient_descent(X, y, theta, lr, num_iter):\n",
    "        for i in range(num_iter):\n",
    "            z = np.dot(X, theta)\n",
    "            h = 1/(1+np.exp(-z))\n",
    "            gradient = np.dot(X.T, (h-y))/y.size\n",
    "            theta -= lr*gradient\n",
    "            \n",
    "        return theta\n",
    "    \n",
    "    # Stochastic Gradient Descent (SGD) using Numba\n",
    "    @staticmethod    \n",
    "    @jit(nopython=True)\n",
    "    def SGD(X, y, theta, lr, num_iter, batch_size, idx):\n",
    "        for i in range(num_iter):\n",
    "            batch_idx = idx[i]\n",
    "            batch_X = X[batch_idx]\n",
    "            batch_y = y[batch_idx]\n",
    "            z = np.dot(batch_X, theta)\n",
    "            h = 1/(1+np.exp(-z))\n",
    "            gradient = np.dot(batch_X.T, (h - batch_y)) / batch_y.size\n",
    "            theta -= lr * gradient\n",
    "            \n",
    "        return theta\n",
    "\n",
    "    # Fit model using CUDA kernel for matrix multiplication\n",
    "    def fit_cuda(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights, allocate memory\n",
    "        TPB=16\n",
    "        X_d = cuda.to_device(np.float32(X))\n",
    "        y_d = cuda.to_device(y.reshape([y.shape[0],1]))\n",
    "        z_d = cuda.to_device(np.zeros([X.shape[0],1], dtype=np.float32))\n",
    "        h_d = cuda.to_device(np.zeros(z_d.shape, dtype=np.float32))\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        theta_d = cuda.to_device(np.zeros((X.shape[1],1), dtype=np.float32))\n",
    "        threadsperblock = (TPB, TPB)\n",
    "        blockspergrid_x = math.ceil(z_h.shape[0] / threadsperblock[0])\n",
    "        blockspergrid_y = math.ceil(z_h.shape[1] / threadsperblock[1])\n",
    "        blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "        \n",
    "        # Gradient descent using CUDA kernel\n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            # Fast matrix multiplication\n",
    "            fast_matmul[blockspergrid, threadsperblock](X, theta_d, z_d)\n",
    "            z = z_d.copy_to_host()\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h.reshape(y.shape) - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            # Print results at step 10000 for verbose\n",
    "            if self.verbose:\n",
    "                if i % 10000 == 0:\n",
    "                    z = np.dot(X, self.theta)\n",
    "                    h = self.__sigmoid(z)\n",
    "                    print(f'Step: {i}')\n",
    "                    print(f'loss: {self.__loss(h, y)}')\n",
    "        \n",
    "    # Fit model using Numba + SGD\n",
    "    def fit_optimized_numba(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        # Gradient descent\n",
    "        rng = np.random.default_rng()\n",
    "        idx = rng.integers(len(X), size=(self.num_iter, 10))\n",
    "        self.theta = self.SGD(X, y, np.zeros(X.shape[1]), self.lr, self.num_iter, self.batch_size, idx)\n",
    "\n",
    "    # Predict probability (class 1)\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        # Add intercept if required\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    # Convert probabilities to predictions\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel function for matrix multiplication\n",
    "@cuda.jit\n",
    "def cuda_dot(A, B, C):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        C[i, j] = 0\n",
    "        for k in range(A.shape[1]):\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "        C[i,j] += C[i,j]\n",
    "\n",
    "# CUDA kernel for activation function\n",
    "@cuda.jit\n",
    "def cuda_sigmoid(z, out):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < z.shape[0] and j < z.shape[1]:\n",
    "        out[i, j] = 1 / (1 + math.exp(-z[i, j]))\n",
    "\n",
    "# CUDA kernel for computing gradient\n",
    "@cuda.jit\n",
    "def cuda_gradient(X, h, y, gradient):\n",
    "    i = cuda.grid(1)\n",
    "    if i < X.shape[1]:\n",
    "        gradient[i] = 0\n",
    "        for j in range(X.shape[0]):\n",
    "            gradient[i] += (h[j, 0] - y[j]) * X[j, i]\n",
    "        gradient[i] /= y.size\n",
    "\n",
    "# CUDA kernel for elementw-wise subtraction\n",
    "@cuda.jit\n",
    "def cuda_subtract(a, b, diff):\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < a.shape[0] and j < b.shape[1]:\n",
    "        diff[i,j] = a[i,j] - b[i,j]\n",
    "            \n",
    "# CUDA kernel for optimized matrix multiplication\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/64198479/13697228 by @RobertCrovella\n",
    "    \"\"\"\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=np.float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=np.float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = np.float32(0.)\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty + i * TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression model based on version\n",
    "def run_logistic_regression(version):\n",
    "    lr = LogisticRegression(lr=learning_rate,\n",
    "                            num_iter=number_iterations,\n",
    "                            batch_size=batch_num,\n",
    "                            verbose=False)\n",
    "    assert version in ['numba', 'optimized_numba', 'cuda']\n",
    "    if version == 'numba':\n",
    "        lr.fit_numba(X_train, y_train)\n",
    "    elif version == 'optimized_numba':\n",
    "        lr.fit_optimized_numba(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate(version):\n",
    "    \n",
    "    # Run and time model\n",
    "    start = time.time()\n",
    "    y_pred = run_logistic_regression(version)\n",
    "    end = time.time()\n",
    "    time_diff = round(end - start, digits)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    auc = round(roc_auc_score(y_test, y_pred), digits)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), digits)\n",
    "    precision = round(precision_score(y_test, y_pred), digits)\n",
    "    recall = round(recall_score(y_test, y_pred), digits)\n",
    "    f1 = round(f1_score(y_test, y_pred), digits)\n",
    "\n",
    "    # Print Results\n",
    "    print(f'Time: {time_diff}')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'AUC: {recall}')\n",
    "    print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47.00324\n",
      "AUC: 0.62521\n",
      "Accuracy: 0.67092\n",
      "Precision: 0.63946\n",
      "AUC: 0.39985\n",
      "F1: 0.49203\n"
     ]
    }
   ],
   "source": [
    "evaluate('numba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.75651\n",
      "AUC: 0.62005\n",
      "Accuracy: 0.66864\n",
      "Precision: 0.64244\n",
      "AUC: 0.38043\n",
      "F1: 0.47788\n"
     ]
    }
   ],
   "source": [
    "evaluate('optimized_numba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: need CUDA GPU to run cuda model\n",
    "#evalute('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrjaUf0u1X6u15lYYZoS4H",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
